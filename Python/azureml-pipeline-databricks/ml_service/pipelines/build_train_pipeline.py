import os
import sys
import base64
import hashlib
import databricks_client
from azureml.core import Workspace
from azureml.pipeline.core import Pipeline, PipelineData
from azureml.pipeline.steps import DatabricksStep
from azureml.core.datastore import Datastore
from azureml.data.data_reference import DataReference
from azure.common.credentials import get_azure_cli_credentials
from azure.mgmt.storage import StorageManagementClient

sys.path.append(os.path.abspath("./ml_service/util"))  # NOQA: E402
from attach_compute import get_databricks_compute, create_databricks_compute
from env_variables import Env


DATABRICKS_RUNTIME_VERSION = "6.2.x-scala2.11"


def upload_notebook(dbricks_client, notebook_folder,
                    notebook_dir, notebook_name):
    """
    Uploads a notebook to databricks.
    """

    # Read notebook file into a Base-64 encoded string
    with open(f"{notebook_dir}/{notebook_name}.py", "r") as file:
        file_content = file.read()
    notebook_content = file_content.encode('utf-8')
    content_b64 = base64.b64encode(notebook_content)
    notebook_checksum = hashlib.sha1(notebook_content).hexdigest()

    notebook_subfolder = f"{notebook_folder}/{notebook_checksum}"
    notebook_path = f"{notebook_subfolder}/{notebook_name}"

    # Create the notebook directory in the Databricks workspace.
    # Will not fail if the directory already exists
    dbricks_client.post(
        'workspace/mkdirs',
        json={
            "path": notebook_subfolder,
        }
    )

    # Import notebook into workspace
    dbricks_client.post(
        'workspace/import',
        json={
            "content": content_b64.decode('ascii'),
            "path": notebook_path,
            "overwrite": True,
            "language": "PYTHON",
            "format": "SOURCE"
        }
    )

    return notebook_path


def get_instance_pool(dbricks_client, pool_name):
    """
    Get the instance pool ID corresponding to an instance pool name.
    Returns None if instance pool with that name was not found.
    """
    # Query API for list of instance pools
    response = dbricks_client.get(
        'instance-pools/list',
    )
    # API quirk: 'instance_pools' element is not returned if
    # there are no instance pools.
    if 'instance_pools' in response:
        for pool in response['instance_pools']:
            if pool["instance_pool_name"] == pool_name:
                return pool["instance_pool_id"]
    return None


def main():
    """
    Builds the Azure ML pipeline for data engineering and model training.
    """
    e = Env()

    # Get Azure machine learning workspace
    aml_workspace = Workspace.get(
        name=e.workspace_name,
        subscription_id=e.subscription_id,
        resource_group=e.resource_group,
    )
    print(aml_workspace)

    # Create a datastore for the training data container
    credentials, subscription = get_azure_cli_credentials()
    storage_client = StorageManagementClient(credentials, subscription)
    training_storage_keys = storage_client.storage_accounts.list_keys(
        e.resource_group, e.training_account_name
    )
    training_datastore = Datastore.register_azure_blob_container(
        workspace=aml_workspace,
        datastore_name="trainingdata",
        container_name="trainingdata",
        account_name=e.training_account_name,
        account_key=training_storage_keys.keys[0].value,
    )

    # Generate Databricks credentials, see https://aka.ms/databricks-aad
    dbricks_region = e.databricks_region
    dbricks_api = f"https://{dbricks_region}.azuredatabricks.net/api/2.0"
    dbricks_client = databricks_client.create(dbricks_api)
    dbricks_client.auth_azuread(
        resource_group=e.resource_group,
        workspace_name=e.databricks_workspace_name)

    # Attach Databricks as Azure ML training compute
    dbricks_compute = get_databricks_compute(
        aml_workspace,
        e.databricks_compute_name,
    )
    if dbricks_compute is None:
        pat_token = dbricks_client.post(
            'token/create',
            json={"comment":
                  f"Azure ML Token generated by Build {e.build_buildid}"}
        )['token_value']
        dbricks_compute = create_databricks_compute(
            aml_workspace,
            e.databricks_compute_name,
            pat_token,
        )

    print("dbricks_compute:")
    print(dbricks_compute)

    # Create Databricks instance pool
    pool_name = "azureml_training"
    instance_pool_id = get_instance_pool(dbricks_client,
                                         pool_name)
    if not instance_pool_id:
        dbricks_client.post(
            'instance-pools/create',
            json={
                "instance_pool_name": pool_name,
                "node_type_id": "Standard_D3_v2",
                "idle_instance_autotermination_minutes": 10,
                "preloaded_spark_versions": [DATABRICKS_RUNTIME_VERSION],
            }
        )
        instance_pool_id = get_instance_pool(dbricks_client,
                                             pool_name)

    notebook_folder = f"/Shared/AzureMLDeployed"
    workspace_datastore = Datastore(aml_workspace, "workspaceblobstore")

    # FEATURE ENGINEERING STEP (DATABRICKS)
    # Create feature engineering pipeline step

    training_data_input = DataReference(
        datastore=training_datastore,
        path_on_datastore="/",
        data_reference_name="training"
    )

    feature_eng_output = PipelineData("feature_engineered",
                                      datastore=workspace_datastore)

    notebook_path = upload_notebook(
        dbricks_client, notebook_folder,
        "code/prepare", "feature_engineering")

    training_dataprep_step = DatabricksStep(
        name="FeatureEngineering",
        inputs=[training_data_input],
        outputs=[feature_eng_output],
        spark_version=DATABRICKS_RUNTIME_VERSION,
        instance_pool_id=instance_pool_id,
        num_workers=3,
        notebook_path=notebook_path,
        run_name="FeatureEngineering",
        compute_target=dbricks_compute,
        allow_reuse=True,
    )

    # You can add Azure ML model training tasks using feature_eng_output as input.
    # ...

    # Create Azure ML Pipeline
    steps = [training_dataprep_step]

    ml_pipeline = Pipeline(workspace=aml_workspace, steps=steps)
    ml_pipeline.validate()
    published_pipeline = ml_pipeline.publish(
        name=e.pipeline_name,
        description="Feature engineering pipeline",
        version=e.build_buildid,
    )
    print(f"Published pipeline: {published_pipeline.name}")
    print(f"for build {published_pipeline.version}")


if __name__ == "__main__":
    main()
